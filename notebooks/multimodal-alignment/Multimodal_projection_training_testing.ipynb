{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7144479,"sourceType":"datasetVersion","datasetId":4109189}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.functional as F\nfrom tqdm import tqdm\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data and create dataloaders","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/modelnet-minimal/consolidated_feat_train.pkl\", \"rb\") as f:\n    all_train_data = pickle.load(f)\nprint(f\"Number of train examples: {len(all_train_data)}\")\n\nwith open(\"/kaggle/input/modelnet-minimal/consolidated_feat_test.pkl\", \"rb\") as f:\n    all_test_data = pickle.load(f)\nprint(f\"Number of test examples: {len(all_test_data)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(all_train_data, batch_size = 256, shuffle = False)\ntest_dataloader = DataLoader(all_test_data, batch_size = 128, shuffle = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define model arch","metadata":{}},{"cell_type":"code","source":"class LinearProjectionHeadBase(nn.Module):\n    def __init__(self, input_emb_size = 1024, output_emb_size = 1024, inter_size_1 = 2048, inter_size_2 = 4096, bottle_size = 8192, dropout_rate = 0.20, device = device):\n        super().__init__()\n        # Initialise parameters\n        self.input_emb_size = input_emb_size\n        self.output_emb_size = output_emb_size\n        self.inter_size_1 = inter_size_1\n        self.inter_size_2 = inter_size_2\n        self.bottle_size = bottle_size\n        self.dropout_rate = dropout_rate\n        self.device = device\n        \n        # Up projection -1\n        self.up1 = nn.Sequential(\n            nn.Linear(self.input_emb_size, self.inter_size_1),\n            nn.PReLU(),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Up projection -2\n        self.up2 = nn.Sequential(\n            nn.Linear(self.inter_size_1, self.inter_size_2),\n            nn.PReLU(),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Bottleneck layer\n        self.bottleneck = nn.Sequential(\n            nn.Linear(self.inter_size_2, self.bottle_size),\n            nn.Tanh(),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Down projection-1\n        self.down1 = nn.Sequential(\n            nn.Linear(self.bottle_size, self.inter_size_2),\n            nn.Tanh(),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Down projection-2\n        self.down2 = nn.Sequential(\n            nn.Linear(self.inter_size_2, self.inter_size_1),\n            nn.Tanh(),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Final projection to output space\n        self.fc = nn.Sequential(\n            nn.Linear(self.inter_size_1, self.output_emb_size),\n        )\n        \n    def forward(self, x):\n        x = x.to(device)\n        x = self.up1(x)\n        x = self.up2(x)\n        x = self.bottleneck(x)\n        x = self.down1(x)\n        x = self.down2(x)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearProjectionHeadLN(nn.Module):\n    def __init__(self, input_emb_size = 1024, output_emb_size = 1024, inter_size_1 = 2048, inter_size_2 = 4096, bottle_size = 8192, dropout_rate = 0.20, device = device):\n        super().__init__()\n        # Initialise parameters\n        self.input_emb_size = input_emb_size\n        self.output_emb_size = output_emb_size\n        self.inter_size_1 = inter_size_1\n        self.inter_size_2 = inter_size_2\n        self.bottle_size = bottle_size\n        self.dropout_rate = dropout_rate\n        self.device = device\n        \n        # Up projection -1\n        self.up1 = nn.Sequential(\n            nn.Linear(self.input_emb_size, self.inter_size_1),\n            nn.PReLU(),\n            nn.LayerNorm(self.inter_size_1),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Up projection -2\n        self.up2 = nn.Sequential(\n            nn.Linear(self.inter_size_1, self.inter_size_2),\n            nn.PReLU(),\n            nn.LayerNorm(self.inter_size_2),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Bottleneck layer\n        self.bottleneck = nn.Sequential(\n            nn.Linear(self.inter_size_2, self.bottle_size),\n            nn.Tanh(),\n            nn.LayerNorm(self.bottle_size),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Down projection-1\n        self.down1 = nn.Sequential(\n            nn.Linear(self.bottle_size, self.inter_size_2),\n            nn.Tanh(),\n            nn.LayerNorm(self.inter_size_2),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Down projection-2\n        self.down2 = nn.Sequential(\n            nn.Linear(self.inter_size_2, self.inter_size_1),\n            nn.Tanh(),\n            nn.LayerNorm(self.inter_size_1),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Final projection to output space\n        self.fc = nn.Sequential(\n            nn.Linear(self.inter_size_1, self.output_emb_size),\n        )\n        \n    def forward(self, x):\n        x = x.to(device)\n        x = self.up1(x)\n        x = self.up2(x)\n        x = self.bottleneck(x)\n        x = self.down1(x)\n        x = self.down2(x)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearProjectionHeadRN(nn.Module):\n    def __init__(self, input_emb_size = 1024, output_emb_size = 1024, inter_size_1 = 2048, inter_size_2 = 4096, bottle_size = 8192, dropout_rate = 0.20, device = device):\n        super().__init__()\n        # Initialise parameters\n        self.input_emb_size = input_emb_size\n        self.output_emb_size = output_emb_size\n        self.inter_size_1 = inter_size_1\n        self.inter_size_2 = inter_size_2\n        self.bottle_size = bottle_size\n        self.dropout_rate = dropout_rate\n        self.device = device\n        \n        # Up projection -1\n        self.up1 = nn.Sequential(\n            nn.Linear(self.input_emb_size, self.inter_size_1),\n            nn.PReLU(),\n            nn.LocalResponseNorm(1),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Up projection -2\n        self.up2 = nn.Sequential(\n            nn.Linear(self.inter_size_1, self.inter_size_2),\n            nn.PReLU(),\n            nn.LocalResponseNorm(1),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Bottleneck layer\n        self.bottleneck = nn.Sequential(\n            nn.Linear(self.inter_size_2, self.bottle_size),\n            nn.Tanh(),\n            nn.LocalResponseNorm(1),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Down projection-1\n        self.down1 = nn.Sequential(\n            nn.Linear(self.bottle_size, self.inter_size_2),\n            nn.Tanh(),\n            nn.LocalResponseNorm(1),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Down projection-2\n        self.down2 = nn.Sequential(\n            nn.Linear(self.inter_size_2, self.inter_size_1),\n            nn.Tanh(),\n            nn.LocalResponseNorm(1),\n            nn.Dropout(p = self.dropout_rate)\n        )\n        \n        # Final projection to output space\n        self.fc = nn.Sequential(\n            nn.Linear(self.inter_size_1, self.output_emb_size),\n        )\n        \n    def forward(self, x):\n        x = x.to(device)\n        x = self.up1(x)\n        x = self.up2(x)\n        x = self.bottleneck(x)\n        x = self.down1(x)\n        x = self.down2(x)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = LinearProjectionHeadBase(device = device).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define loss function and optimiser","metadata":{}},{"cell_type":"code","source":"criterion = nn.L1Loss().to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\noptimiser = optim.Adam(net.parameters(), lr = 0.005)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training loop","metadata":{}},{"cell_type":"code","source":"NUM_EPOCHS = 1_000\nall_scores = []\nall_test_scores = []\nimport torch.nn.functional as F\n\nfor epoch in tqdm(range(NUM_EPOCHS)):\n    # Training loop\n    net.train()\n    scores = []\n    for i, data in enumerate(train_dataloader, 0):\n        # Split the input data into point cloud and image data\n        pcl, img = data\n        X = torch.Tensor(pcl).to(device)\n        y = torch.Tensor(img).to(device)\n        \n        # Zero the optim\n        optimiser.zero_grad()\n        \n        # Forward + Backward + Optimise\n        X_t = net(X)\n        X_t = torch.Tensor(X_t).to(device)\n        y.to(device)\n        loss = criterion(X_t, y)\n        scores.append(loss)\n        \n        loss.backward()\n        optimiser.step()\n    \n    scores = torch.Tensor(scores)\n    scores = np.array(scores.detach().cpu())\n    all_scores.append((sum(scores)/len(scores))) \n    \n    # Testing loop\n    test_scores = []\n    net.eval()\n    with torch.no_grad():\n        for i, data in enumerate(test_dataloader, 0):\n            # Split the input data into point cloud and image data\n            pcl, img = data\n            X = torch.Tensor(pcl).to(device)\n            y = torch.Tensor(img).to(device)\n            # Forward\n            X_t = net(X)\n            X_t = torch.Tensor(X_t).to(device)\n            y.to(device)\n            loss = criterion(X_t, y)\n            test_scores.append(loss)\n\n\n        test_scores = torch.Tensor(test_scores)\n        test_scores = np.array(test_scores.detach().cpu())\n        all_test_scores.append((sum(test_scores)/len(test_scores))) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nall_train_scores = all_scores\n\n_ = plt.plot(all_train_scores)\n_ = plt.plot(all_test_scores)\nplt.xlabel('Epoch')\nplt.ylabel('L1Loss Scores')\nplt.title('LinearProjectionHead Scores')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_train_scores[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_test_scores[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the model","metadata":{}},{"cell_type":"code","source":"PATH = f\"mapping_base_Modelnet_{NUM_EPOCHS}.pth\"\ntorch.save(net.state_dict(), PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Download model](./mapping_base_Modelnet_1000.pth)","metadata":{}}]}