{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7138809,"sourceType":"datasetVersion","datasetId":4119419}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport torch\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:04:35.326117Z","iopub.execute_input":"2023-12-06T17:04:35.326642Z","iopub.status.idle":"2023-12-06T17:04:36.745476Z","shell.execute_reply.started":"2023-12-06T17:04:35.326590Z","shell.execute_reply":"2023-12-06T17:04:36.744670Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Load the `laion/CLIP-ViT-H-14-laion2B-s32B-b79K` model","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    Use the `transformers` library to instantiate and get inference scores for different variants of the `clip-vit` family of zero-shot-classification models\n@ref https://codeandlife.com/2023/01/26/mastering-the-huggingface-clip-model-how-to-extract-embeddings-and-calculate-similarity-for-text-and-images/\n\"\"\"\nfrom PIL import Image\nfrom transformers import AutoProcessor, CLIPModel, AutoTokenizer\n\n# Model variant\nmodel_variant = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n\n# Load model, tokeniser, and processor\nmodel = CLIPModel.from_pretrained(model_variant)\nprocessor = AutoProcessor.from_pretrained(model_variant)\ntokenizer = AutoTokenizer.from_pretrained(model_variant)\n\n# Get image/text similarity softmax output\ndef image_text_relevance(image_path:str, text_choices:list[str]):\n    global processor, model\n    img = Image.open(image_path)\n    inputs = processor(\n        text = text_choices,\n        images = img,\n        return_tensors = \"pt\",\n        padding = True\n    )\n\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n    return probs\n\n# Get image feature vector\ndef image_features(image_path:str):\n    global model, processor\n    img = Image.open(image_path)\n    inputs = processor(\n        images = img,\n        return_tensors = \"pt\",\n        padding = True,\n        device=0\n    ).to(\"cuda:0\")\n    image_features = model.get_image_features(**inputs) # image features\n    return image_features","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-06T17:05:39.073228Z","iopub.execute_input":"2023-12-06T17:05:39.073631Z","iopub.status.idle":"2023-12-06T17:05:52.314207Z","shell.execute_reply.started":"2023-12-06T17:05:39.073600Z","shell.execute_reply":"2023-12-06T17:05:52.313110Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to(\"cuda:0\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:05:52.316014Z","iopub.execute_input":"2023-12-06T17:05:52.316341Z","iopub.status.idle":"2023-12-06T17:05:53.276959Z","shell.execute_reply.started":"2023-12-06T17:05:52.316315Z","shell.execute_reply":"2023-12-06T17:05:53.275996Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"CLIPModel(\n  (text_model): CLIPTextTransformer(\n    (embeddings): CLIPTextEmbeddings(\n      (token_embedding): Embedding(49408, 1024)\n      (position_embedding): Embedding(77, 1024)\n    )\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-23): 24 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (vision_model): CLIPVisionTransformer(\n    (embeddings): CLIPVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n      (position_embedding): Embedding(257, 1280)\n    )\n    (pre_layrnorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-31): 32 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (layer_norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          )\n          (layer_norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n  (visual_projection): Linear(in_features=1280, out_features=1024, bias=False)\n  (text_projection): Linear(in_features=1024, out_features=1024, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Load the input `.png` files","metadata":{}},{"cell_type":"code","source":"import glob\n\nall_png_files = glob.glob(\"/kaggle/input/pointnet-merged/original-image-individual/original-image-individual/rendered-original/*.png\")\nprint(f\"Total number of examples: {len(all_png_files)}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:05:53.278263Z","iopub.execute_input":"2023-12-06T17:05:53.278632Z","iopub.status.idle":"2023-12-06T17:05:53.295448Z","shell.execute_reply.started":"2023-12-06T17:05:53.278598Z","shell.execute_reply":"2023-12-06T17:05:53.294539Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Total number of examples: 2468\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Method to get 1024-dim features ","metadata":{}},{"cell_type":"code","source":"def global_features(img_path):\n    # Get the model features\n    feat = image_features(img_path)\n    return feat","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:05:56.577168Z","iopub.execute_input":"2023-12-06T17:05:56.578280Z","iopub.status.idle":"2023-12-06T17:05:56.582453Z","shell.execute_reply.started":"2023-12-06T17:05:56.578243Z","shell.execute_reply":"2023-12-06T17:05:56.581522Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Generate and save embeddings for ModelNet","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\n\nall_model_outputs = {}\n\nfor i in tqdm(range(len(all_png_files))):\n    # Get file name identifier\n    full_file_name = all_png_files[i]\n    filtered_file_name = full_file_name.split(\"/\")[-1].replace(\"_image.png\",\"\")\n    \n    # Get output tensor\n    feat = global_features(full_file_name)\n    feat_cpu = feat.detach().cpu()\n    \n    # Save model outputs\n    if filtered_file_name not in all_model_outputs:\n        all_model_outputs[filtered_file_name] = feat_cpu","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:05:58.603156Z","iopub.execute_input":"2023-12-06T17:05:58.603912Z","iopub.status.idle":"2023-12-06T17:10:37.964842Z","shell.execute_reply.started":"2023-12-06T17:05:58.603876Z","shell.execute_reply":"2023-12-06T17:10:37.963832Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 2468/2468 [04:39<00:00,  8.83it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Save the embeddings","metadata":{}},{"cell_type":"code","source":"assert len(all_model_outputs.keys()) == len(all_png_files)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:10:52.647148Z","iopub.execute_input":"2023-12-06T17:10:52.648096Z","iopub.status.idle":"2023-12-06T17:10:52.652205Z","shell.execute_reply.started":"2023-12-06T17:10:52.648061Z","shell.execute_reply":"2023-12-06T17:10:52.651248Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open(\"clip_image_global_features_full.pkl\", \"wb\") as f:\n    pickle.dump(all_model_outputs, f)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:10:54.629333Z","iopub.execute_input":"2023-12-06T17:10:54.630246Z","iopub.status.idle":"2023-12-06T17:10:54.813072Z","shell.execute_reply.started":"2023-12-06T17:10:54.630210Z","shell.execute_reply":"2023-12-06T17:10:54.812324Z"},"trusted":true},"execution_count":12,"outputs":[]}]}