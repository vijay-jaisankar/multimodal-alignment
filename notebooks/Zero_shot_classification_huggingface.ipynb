{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7123651,"sourceType":"datasetVersion","datasetId":4109189}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Zero-shot classification Evaluation","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T18:55:17.076928Z","iopub.execute_input":"2023-12-05T18:55:17.077689Z","iopub.status.idle":"2023-12-05T18:55:17.090985Z","shell.execute_reply.started":"2023-12-05T18:55:17.077645Z","shell.execute_reply":"2023-12-05T18:55:17.089981Z"},"trusted":true},"execution_count":155,"outputs":[{"name":"stdout","text":"/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_0.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_12.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_11.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_16.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_8.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_19.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_22.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_20.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_34.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_26.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_10.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_27.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_15.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_9.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_24.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_17.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_3.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_35.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_23.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_30.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_21.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_1.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_5.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_38.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_18.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_28.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_37.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_7.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_6.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_31.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_13.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_4.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_39.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_2.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_29.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_25.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_14.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_32.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_36.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-set-np/testmin_33.npy\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_7_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_6_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_39_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_5_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_17_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_31_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_29_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_18_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_23_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_21_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_32_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_20_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_24_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_35_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_12_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_11_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_1_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_4_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_19_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_14_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_13_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_37_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_16_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_38_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_3_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_36_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_26_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_8_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_28_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_30_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_9_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_33_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_25_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_27_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_2_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_0_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_10_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_15_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_22_image.png\n/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_34_image.png\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Candidate choices\n- https://huggingface.co/openai/clip-vit-base-patch32 \n- https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K\n- https://huggingface.co/openai/clip-vit-large-patch14\n- https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K","metadata":{}},{"cell_type":"code","source":"sample_image_path = \"/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_10_image.png\"","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:55:17.092122Z","iopub.execute_input":"2023-12-05T18:55:17.092409Z","iopub.status.idle":"2023-12-05T18:55:17.106139Z","shell.execute_reply.started":"2023-12-05T18:55:17.092383Z","shell.execute_reply":"2023-12-05T18:55:17.105296Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Use the `transformers` library to instantiate and get inference scores for different variants of the `clip-vit` family of zero-shot-classification models\n@ref https://codeandlife.com/2023/01/26/mastering-the-huggingface-clip-model-how-to-extract-embeddings-and-calculate-similarity-for-text-and-images/\n\"\"\"\nfrom PIL import Image\nfrom transformers import AutoProcessor, CLIPModel, AutoTokenizer\n\n# Model variant\nmodel_variant = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n\n# Load model, tokeniser, and processor\nmodel = CLIPModel.from_pretrained(model_variant)\nprocessor = AutoProcessor.from_pretrained(model_variant)\ntokenizer = AutoTokenizer.from_pretrained(model_variant)\n\n# Get image/text similarity softmax output\ndef image_text_relevance(image_path:str, text_choices:list[str]):\n    global processor, model\n    img = Image.open(image_path)\n    inputs = processor(\n        text = text_choices,\n        images = img,\n        return_tensors = \"pt\",\n        padding = True\n    )\n\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n    return probs\n\n# Get image feature vector\ndef image_features(image_path:str):\n    global model, processor\n    img = Image.open(image_path)\n    inputs = processor(\n        images = image,\n        return_tensors = \"pt\",\n        padding = True\n    )\n    image_features = model.get_image_features(**inputs) # image features\n    return image_features","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:55:17.108110Z","iopub.execute_input":"2023-12-05T18:55:17.108385Z","iopub.status.idle":"2023-12-05T18:55:35.357444Z","shell.execute_reply.started":"2023-12-05T18:55:17.108360Z","shell.execute_reply":"2023-12-05T18:55:35.356627Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stderr","text":"`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n","output_type":"stream"}]},{"cell_type":"code","source":"X = image_text_relevance(sample_image_path, [\"A model of a cup\", \"A model of a ball\", \"A model of a train\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:55:35.358623Z","iopub.execute_input":"2023-12-05T18:55:35.358944Z","iopub.status.idle":"2023-12-05T18:55:37.555730Z","shell.execute_reply.started":"2023-12-05T18:55:35.358917Z","shell.execute_reply":"2023-12-05T18:55:37.554776Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:55:37.557079Z","iopub.execute_input":"2023-12-05T18:55:37.557398Z","iopub.status.idle":"2023-12-05T18:55:37.607263Z","shell.execute_reply.started":"2023-12-05T18:55:37.557370Z","shell.execute_reply":"2023-12-05T18:55:37.606339Z"},"trusted":true},"execution_count":159,"outputs":[{"execution_count":159,"output_type":"execute_result","data":{"text/plain":"tensor([[9.9977e-01, 2.3415e-04, 1.4951e-09]], grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Construct text prompts","metadata":{}},{"cell_type":"code","source":"label_to_names = {0: 'airplane',\n       1: 'bathtub',\n       2: 'bed',\n       3: 'bench',\n       4: 'bookshelf',\n       5: 'bottle',\n       6: 'bowl',\n       7: 'car',\n       8: 'chair',\n       9: 'cone',\n       10: 'cup',\n       11: 'curtain',\n       12: 'desk',\n       13: 'door',\n       14: 'dresser',\n       15: 'flower_pot',\n       16: 'glass_box',\n       17: 'guitar',\n       18: 'keyboard',\n       19: 'lamp',\n       20: 'laptop',\n       21: 'mantel',\n       22: 'monitor',\n       23: 'night_stand',\n       24: 'person',\n       25: 'piano',\n       26: 'plant',\n       27: 'radio',\n       28: 'range_hood',\n       29: 'sink',\n       30: 'sofa',\n       31: 'stairs',\n       32: 'stool',\n       33: 'table',\n       34: 'tent',\n       35: 'toilet',\n       36: 'tv_stand',\n       37: 'vase',\n       38: 'wardrobe',\n       39: 'xbox'\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:55:37.640344Z","iopub.execute_input":"2023-12-05T18:55:37.640671Z","iopub.status.idle":"2023-12-05T18:55:37.648996Z","shell.execute_reply.started":"2023-12-05T18:55:37.640646Z","shell.execute_reply":"2023-12-05T18:55:37.648065Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"all_text_prompts = []\n\nfor i in range(40):\n    base_prompt = label_to_names[i]\n    full_prompt = f\"A model of {base_prompt}\"\n    all_text_prompts.append(full_prompt)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:55:37.650098Z","iopub.execute_input":"2023-12-05T18:55:37.650434Z","iopub.status.idle":"2023-12-05T18:55:37.663319Z","shell.execute_reply.started":"2023-12-05T18:55:37.650400Z","shell.execute_reply":"2023-12-05T18:55:37.662424Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"def top_1_individual(img_file_path, text_prompts):\n    # Find the file index from the full file path\n    def find_index_from_filename(img_file_path):\n        # sample_image_path = \"/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_10_image.png\"\n        base_file_path = img_file_path.split(\"/\")[-1].replace(\".png\",\"\")\n        index = int(base_file_path.split(\"_\")[1])\n        return index\n    \n    # Find the argmax in the output of the softmax function\n    def find_index_softmax(img_file_path, text_prompts):\n        # Get the model outputs\n        X = image_text_relevance(img_file_path, text_prompts)\n        # Get the argmax\n        y = torch.argmax(X)\n        index = y.detach().cpu()\n        return index\n    \n    # Comparison cell\n    index_1 = find_index_from_filename(img_file_path)\n    index_2 = find_index_softmax(img_file_path, text_prompts)\n    \n    if index_1 is None or index_2 is None:\n        return 0\n    \n    if int(index_1) == int(index_2):\n        return 1\n    else:\n        return 0    ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:55:37.664369Z","iopub.execute_input":"2023-12-05T18:55:37.664654Z","iopub.status.idle":"2023-12-05T18:55:37.673716Z","shell.execute_reply.started":"2023-12-05T18:55:37.664631Z","shell.execute_reply":"2023-12-05T18:55:37.672907Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"import glob\nfrom tqdm import tqdm\nall_input_images_test = (glob.glob(\"/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/*.png\"))\n\ntotal_score = 0\nfor img in tqdm(all_input_images_test):\n    total_score += top_1_individual(img, all_text_prompts)\n    \nprint(total_score)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:55:37.674735Z","iopub.execute_input":"2023-12-05T18:55:37.675040Z","iopub.status.idle":"2023-12-05T18:57:37.844006Z","shell.execute_reply.started":"2023-12-05T18:55:37.675016Z","shell.execute_reply":"2023-12-05T18:57:37.843049Z"},"trusted":true},"execution_count":166,"outputs":[{"name":"stderr","text":"100%|██████████| 40/40 [02:00<00:00,  3.00s/it]","output_type":"stream"},{"name":"stdout","text":"12\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"total_score/40","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:57:37.845319Z","iopub.execute_input":"2023-12-05T18:57:37.845624Z","iopub.status.idle":"2023-12-05T18:57:37.851838Z","shell.execute_reply.started":"2023-12-05T18:57:37.845598Z","shell.execute_reply":"2023-12-05T18:57:37.850980Z"},"trusted":true},"execution_count":167,"outputs":[{"execution_count":167,"output_type":"execute_result","data":{"text/plain":"0.3"},"metadata":{}}]},{"cell_type":"code","source":"def top_k_individual(img_file_path, text_prompts, k):\n    # Find the file index from the full file path\n    def find_index_from_filename(img_file_path):\n        # sample_image_path = \"/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/testmin_10_image.png\"\n        base_file_path = img_file_path.split(\"/\")[-1].replace(\".png\",\"\")\n        index = int(base_file_path.split(\"_\")[1])\n        return index\n    \n    # Find the k argmax indices in the output of the softmax function\n    def find_index_softmax(img_file_path, text_prompts):\n        # Get the model outputs\n        X = image_text_relevance(img_file_path, text_prompts)\n        # Get the argmax\n        y = torch.topk(X, int(k))\n        z = np.array(y.indices.detach().cpu())[0]\n        return z.tolist()\n    \n    # Comparison cell\n    index_1 = find_index_from_filename(img_file_path)\n    index_2 = find_index_softmax(img_file_path, text_prompts)\n    \n    if index_1 is None or index_2 is None:\n        return 0\n    \n    if int(index_1) in index_2:\n        return 1\n    else:\n        return 0    ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:57:37.853245Z","iopub.execute_input":"2023-12-05T18:57:37.853625Z","iopub.status.idle":"2023-12-05T18:57:37.864225Z","shell.execute_reply.started":"2023-12-05T18:57:37.853591Z","shell.execute_reply":"2023-12-05T18:57:37.863257Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"import glob\nfrom tqdm import tqdm\nall_input_images_test = (glob.glob(\"/kaggle/input/modelnet-minimal/minimal-test-set/minimal-test-img/*.png\"))\n\ntotal_score = 0\nfor img in tqdm(all_input_images_test):\n    total_score += top_k_individual(img, all_text_prompts, 3)\n    \nprint(total_score)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T19:03:37.720765Z","iopub.execute_input":"2023-12-05T19:03:37.721226Z","iopub.status.idle":"2023-12-05T19:05:42.905091Z","shell.execute_reply.started":"2023-12-05T19:03:37.721192Z","shell.execute_reply":"2023-12-05T19:05:42.904090Z"},"trusted":true},"execution_count":171,"outputs":[{"name":"stderr","text":"100%|██████████| 40/40 [02:05<00:00,  3.13s/it]","output_type":"stream"},{"name":"stdout","text":"16\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"total_score/40","metadata":{"execution":{"iopub.status.busy":"2023-12-05T19:05:42.907349Z","iopub.execute_input":"2023-12-05T19:05:42.908016Z","iopub.status.idle":"2023-12-05T19:05:42.914477Z","shell.execute_reply.started":"2023-12-05T19:05:42.907976Z","shell.execute_reply":"2023-12-05T19:05:42.913535Z"},"trusted":true},"execution_count":172,"outputs":[{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"0.4"},"metadata":{}}]}]}